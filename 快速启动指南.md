# 快速启动指南 - 大模型+小模型模式

## 一键启动步骤

### 1. 检查前置条件

```bash
# 检查 Ollama 是否运行
curl http://localhost:11434/api/tags

# 如果没有运行，启动 Ollama
ollama serve
```

### 2. 确认小模型已下载

```bash
# 检查模型是否存在
ollama list | grep llama3.2-vision

# 如果不存在，下载模型
ollama pull llama3.2-vision
```

### 3. 运行项目

```bash
# 基本运行（使用 config/config.json 中的配置）
video-analyzer your_video.mp4

# 指定配置文件目录
video-analyzer your_video.mp4 --config config

# 带自定义提示词
video-analyzer your_video.mp4 --prompt "这个视频的主要内容是什么？"
```

## 配置说明

配置文件 `config/config.json` 已启用两阶段分析：

- ✅ **小模型**：本地 Ollama `llama3.2-vision`（零成本）
- ✅ **大模型**：API `qwen-vl-max`（用于深度分析）
- ✅ **重要性阈值**：6 分
- ✅ **最大深度分析帧数**：10 帧

## 运行示例

```bash
# 示例 1：分析视频
video-analyzer sample.mp4

# 示例 2：限制帧数（测试用）
video-analyzer sample.mp4 --max-frames 5

# 示例 3：自定义提示
video-analyzer sample.mp4 --prompt "视频中有哪些关键场景？"

# 示例 4：从第2阶段开始（如果已经提取了帧）
video-analyzer sample.mp4 --start-stage 2
```

## 查看结果

运行完成后，结果保存在 `output/analysis.json`：

```bash
# 查看结果
cat output/analysis.json | python -m json.tool

# 或者用编辑器打开
notepad output/analysis.json  # Windows
```

## 验证配置

运行测试脚本验证配置是否正确：

```bash
python test_two_stage.py
```

## 常见问题

### Q: Ollama 连接失败？
A: 确保 Ollama 服务正在运行：`ollama serve`

### Q: 模型未找到？
A: 下载模型：`ollama pull llama3.2-vision`

### Q: API 调用失败？
A: 检查 `config/config.json` 中的 API key 和 URL 是否正确

### Q: 如何调整筛选严格度？
A: 修改 `config/config.json` 中的 `importance_threshold`（降低=更宽松，提高=更严格）

## 下一步

- 查看详细文档：[使用大模型加小模型模式.md](使用大模型加小模型模式.md)
- 查看两阶段分析文档：[docs/TWO_STAGE_ANALYSIS.md](docs/TWO_STAGE_ANALYSIS.md)
- 查看使用指南：[docs/USAGES.md](docs/USAGES.md)





